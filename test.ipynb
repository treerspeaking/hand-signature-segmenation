{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b230c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "pt = torch.tensor([[\n",
    "    [\n",
    "    [0.7, 0.8],\n",
    "    [0.5, 0.4],\n",
    "    [0.5, 0.4],\n",
    "    ],\n",
    "    [\n",
    "    [0.2, 0.4],\n",
    "    [0.5, 0.2],\n",
    "    [0.5, 0.4],\n",
    "    ],\n",
    "                   ]]).float()\n",
    "gt = torch.tensor([[\n",
    "    [\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    [1, 1]\n",
    "    ],\n",
    "    [\n",
    "    [1, 1],\n",
    "    [1, 1],\n",
    "    [1, 1]\n",
    "    ],\n",
    "                   ]], dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abfa0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1, 0],\n",
      "          [1, 1],\n",
      "          [1, 1]],\n",
      "\n",
      "         [[1, 1],\n",
      "          [1, 1],\n",
      "          [1, 1]]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Sigmoid, BCELoss\n",
    "\n",
    "m = Sigmoid()\n",
    "bce = BCELoss(reduction='none')\n",
    "print(gt)\n",
    "# loss = gt * torch.log(pt) * (1 - pt) ** 2\n",
    "# loss = bce(pt, gt).mean()\n",
    "# print(loss)\n",
    "# loss = gt * torch.log(pt) \n",
    "# print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23171dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.7000, 0.8000],\n",
       "          [0.5000, 0.4000],\n",
       "          [0.5000, 0.4000]],\n",
       "\n",
       "         [[0.2000, 0.4000],\n",
       "          [0.5000, 0.2000],\n",
       "          [0.5000, 0.4000]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df5efad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4674322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# F.\n",
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "\n",
    "c = list(zip(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36b46b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "(2, 5)\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "for i in c:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "535d86f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "(2, 5)\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "for i in c:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "627bdeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "# first compute statistics for true positives, false positives, false negative and\n",
    "# true negative \"pixels\"\n",
    "tp, fp, fn, tn = smp.metrics.get_stats(pt, gt, mode='multilabel', threshold=0.5)\n",
    "\n",
    "# then compute metrics with required reduction (see metric docs)\n",
    "iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"none\",)\n",
    "f1_score = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"none\")\n",
    "f2_score = smp.metrics.fbeta_score(tp, fp, fn, tn, beta=2, reduction=\"micro\")\n",
    "accuracy = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"macro\")\n",
    "recall = smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro-imagewise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d67b206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 2]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f402fb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.3333]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7e72aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6667, 0.5000]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2146cc76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4167)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1abf4058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 2]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "717a2bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Checkpoint Keys ===\n",
      "- epoch\n",
      "- global_step\n",
      "- pytorch-lightning_version\n",
      "- state_dict\n",
      "- loops\n",
      "- callbacks\n",
      "- optimizer_states\n",
      "- lr_schedulers\n",
      "- MixedPrecision\n",
      "- hparams_name\n",
      "- hyper_parameters\n",
      "\n",
      "=== State Dict Keys (Model Weights) ===\n",
      "- model.encoder.features.0.0.weight: torch.Size([32, 3, 3, 3])\n",
      "- model.encoder.features.0.1.weight: torch.Size([32])\n",
      "- model.encoder.features.0.1.bias: torch.Size([32])\n",
      "- model.encoder.features.0.1.running_mean: torch.Size([32])\n",
      "- model.encoder.features.0.1.running_var: torch.Size([32])\n",
      "- model.encoder.features.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.1.conv.0.0.weight: torch.Size([32, 1, 3, 3])\n",
      "- model.encoder.features.1.conv.0.1.weight: torch.Size([32])\n",
      "- model.encoder.features.1.conv.0.1.bias: torch.Size([32])\n",
      "- model.encoder.features.1.conv.0.1.running_mean: torch.Size([32])\n",
      "- model.encoder.features.1.conv.0.1.running_var: torch.Size([32])\n",
      "- model.encoder.features.1.conv.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.1.conv.1.weight: torch.Size([16, 32, 1, 1])\n",
      "- model.encoder.features.1.conv.2.weight: torch.Size([16])\n",
      "- model.encoder.features.1.conv.2.bias: torch.Size([16])\n",
      "- model.encoder.features.1.conv.2.running_mean: torch.Size([16])\n",
      "- model.encoder.features.1.conv.2.running_var: torch.Size([16])\n",
      "- model.encoder.features.1.conv.2.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.2.conv.0.0.weight: torch.Size([96, 16, 1, 1])\n",
      "- model.encoder.features.2.conv.0.1.weight: torch.Size([96])\n",
      "- model.encoder.features.2.conv.0.1.bias: torch.Size([96])\n",
      "- model.encoder.features.2.conv.0.1.running_mean: torch.Size([96])\n",
      "- model.encoder.features.2.conv.0.1.running_var: torch.Size([96])\n",
      "- model.encoder.features.2.conv.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.2.conv.1.0.weight: torch.Size([96, 1, 3, 3])\n",
      "- model.encoder.features.2.conv.1.1.weight: torch.Size([96])\n",
      "- model.encoder.features.2.conv.1.1.bias: torch.Size([96])\n",
      "- model.encoder.features.2.conv.1.1.running_mean: torch.Size([96])\n",
      "- model.encoder.features.2.conv.1.1.running_var: torch.Size([96])\n",
      "- model.encoder.features.2.conv.1.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.2.conv.2.weight: torch.Size([24, 96, 1, 1])\n",
      "- model.encoder.features.2.conv.3.weight: torch.Size([24])\n",
      "- model.encoder.features.2.conv.3.bias: torch.Size([24])\n",
      "- model.encoder.features.2.conv.3.running_mean: torch.Size([24])\n",
      "- model.encoder.features.2.conv.3.running_var: torch.Size([24])\n",
      "- model.encoder.features.2.conv.3.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.3.conv.0.0.weight: torch.Size([144, 24, 1, 1])\n",
      "- model.encoder.features.3.conv.0.1.weight: torch.Size([144])\n",
      "- model.encoder.features.3.conv.0.1.bias: torch.Size([144])\n",
      "- model.encoder.features.3.conv.0.1.running_mean: torch.Size([144])\n",
      "- model.encoder.features.3.conv.0.1.running_var: torch.Size([144])\n",
      "- model.encoder.features.3.conv.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.3.conv.1.0.weight: torch.Size([144, 1, 3, 3])\n",
      "- model.encoder.features.3.conv.1.1.weight: torch.Size([144])\n",
      "- model.encoder.features.3.conv.1.1.bias: torch.Size([144])\n",
      "- model.encoder.features.3.conv.1.1.running_mean: torch.Size([144])\n",
      "- model.encoder.features.3.conv.1.1.running_var: torch.Size([144])\n",
      "- model.encoder.features.3.conv.1.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.3.conv.2.weight: torch.Size([24, 144, 1, 1])\n",
      "- model.encoder.features.3.conv.3.weight: torch.Size([24])\n",
      "- model.encoder.features.3.conv.3.bias: torch.Size([24])\n",
      "- model.encoder.features.3.conv.3.running_mean: torch.Size([24])\n",
      "- model.encoder.features.3.conv.3.running_var: torch.Size([24])\n",
      "- model.encoder.features.3.conv.3.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.4.conv.0.0.weight: torch.Size([144, 24, 1, 1])\n",
      "- model.encoder.features.4.conv.0.1.weight: torch.Size([144])\n",
      "- model.encoder.features.4.conv.0.1.bias: torch.Size([144])\n",
      "- model.encoder.features.4.conv.0.1.running_mean: torch.Size([144])\n",
      "- model.encoder.features.4.conv.0.1.running_var: torch.Size([144])\n",
      "- model.encoder.features.4.conv.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.4.conv.1.0.weight: torch.Size([144, 1, 3, 3])\n",
      "- model.encoder.features.4.conv.1.1.weight: torch.Size([144])\n",
      "- model.encoder.features.4.conv.1.1.bias: torch.Size([144])\n",
      "- model.encoder.features.4.conv.1.1.running_mean: torch.Size([144])\n",
      "- model.encoder.features.4.conv.1.1.running_var: torch.Size([144])\n",
      "- model.encoder.features.4.conv.1.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.4.conv.2.weight: torch.Size([32, 144, 1, 1])\n",
      "- model.encoder.features.4.conv.3.weight: torch.Size([32])\n",
      "- model.encoder.features.4.conv.3.bias: torch.Size([32])\n",
      "- model.encoder.features.4.conv.3.running_mean: torch.Size([32])\n",
      "- model.encoder.features.4.conv.3.running_var: torch.Size([32])\n",
      "- model.encoder.features.4.conv.3.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.5.conv.0.0.weight: torch.Size([192, 32, 1, 1])\n",
      "- model.encoder.features.5.conv.0.1.weight: torch.Size([192])\n",
      "- model.encoder.features.5.conv.0.1.bias: torch.Size([192])\n",
      "- model.encoder.features.5.conv.0.1.running_mean: torch.Size([192])\n",
      "- model.encoder.features.5.conv.0.1.running_var: torch.Size([192])\n",
      "- model.encoder.features.5.conv.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.5.conv.1.0.weight: torch.Size([192, 1, 3, 3])\n",
      "- model.encoder.features.5.conv.1.1.weight: torch.Size([192])\n",
      "- model.encoder.features.5.conv.1.1.bias: torch.Size([192])\n",
      "- model.encoder.features.5.conv.1.1.running_mean: torch.Size([192])\n",
      "- model.encoder.features.5.conv.1.1.running_var: torch.Size([192])\n",
      "- model.encoder.features.5.conv.1.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.5.conv.2.weight: torch.Size([32, 192, 1, 1])\n",
      "- model.encoder.features.5.conv.3.weight: torch.Size([32])\n",
      "- model.encoder.features.5.conv.3.bias: torch.Size([32])\n",
      "- model.encoder.features.5.conv.3.running_mean: torch.Size([32])\n",
      "- model.encoder.features.5.conv.3.running_var: torch.Size([32])\n",
      "- model.encoder.features.5.conv.3.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.6.conv.0.0.weight: torch.Size([192, 32, 1, 1])\n",
      "- model.encoder.features.6.conv.0.1.weight: torch.Size([192])\n",
      "- model.encoder.features.6.conv.0.1.bias: torch.Size([192])\n",
      "- model.encoder.features.6.conv.0.1.running_mean: torch.Size([192])\n",
      "- model.encoder.features.6.conv.0.1.running_var: torch.Size([192])\n",
      "- model.encoder.features.6.conv.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.6.conv.1.0.weight: torch.Size([192, 1, 3, 3])\n",
      "- model.encoder.features.6.conv.1.1.weight: torch.Size([192])\n",
      "- model.encoder.features.6.conv.1.1.bias: torch.Size([192])\n",
      "- model.encoder.features.6.conv.1.1.running_mean: torch.Size([192])\n",
      "- model.encoder.features.6.conv.1.1.running_var: torch.Size([192])\n",
      "- model.encoder.features.6.conv.1.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.6.conv.2.weight: torch.Size([32, 192, 1, 1])\n",
      "- model.encoder.features.6.conv.3.weight: torch.Size([32])\n",
      "- model.encoder.features.6.conv.3.bias: torch.Size([32])\n",
      "- model.encoder.features.6.conv.3.running_mean: torch.Size([32])\n",
      "- model.encoder.features.6.conv.3.running_var: torch.Size([32])\n",
      "- model.encoder.features.6.conv.3.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.7.conv.0.0.weight: torch.Size([192, 32, 1, 1])\n",
      "- model.encoder.features.7.conv.0.1.weight: torch.Size([192])\n",
      "- model.encoder.features.7.conv.0.1.bias: torch.Size([192])\n",
      "- model.encoder.features.7.conv.0.1.running_mean: torch.Size([192])\n",
      "- model.encoder.features.7.conv.0.1.running_var: torch.Size([192])\n",
      "- model.encoder.features.7.conv.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.7.conv.1.0.weight: torch.Size([192, 1, 3, 3])\n",
      "- model.encoder.features.7.conv.1.1.weight: torch.Size([192])\n",
      "- model.encoder.features.7.conv.1.1.bias: torch.Size([192])\n",
      "- model.encoder.features.7.conv.1.1.running_mean: torch.Size([192])\n",
      "- model.encoder.features.7.conv.1.1.running_var: torch.Size([192])\n",
      "- model.encoder.features.7.conv.1.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.7.conv.2.weight: torch.Size([64, 192, 1, 1])\n",
      "- model.encoder.features.7.conv.3.weight: torch.Size([64])\n",
      "- model.encoder.features.7.conv.3.bias: torch.Size([64])\n",
      "- model.encoder.features.7.conv.3.running_mean: torch.Size([64])\n",
      "- model.encoder.features.7.conv.3.running_var: torch.Size([64])\n",
      "- model.encoder.features.7.conv.3.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.8.conv.0.0.weight: torch.Size([384, 64, 1, 1])\n",
      "- model.encoder.features.8.conv.0.1.weight: torch.Size([384])\n",
      "- model.encoder.features.8.conv.0.1.bias: torch.Size([384])\n",
      "- model.encoder.features.8.conv.0.1.running_mean: torch.Size([384])\n",
      "- model.encoder.features.8.conv.0.1.running_var: torch.Size([384])\n",
      "- model.encoder.features.8.conv.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.8.conv.1.0.weight: torch.Size([384, 1, 3, 3])\n",
      "- model.encoder.features.8.conv.1.1.weight: torch.Size([384])\n",
      "- model.encoder.features.8.conv.1.1.bias: torch.Size([384])\n",
      "- model.encoder.features.8.conv.1.1.running_mean: torch.Size([384])\n",
      "- model.encoder.features.8.conv.1.1.running_var: torch.Size([384])\n",
      "- model.encoder.features.8.conv.1.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.8.conv.2.weight: torch.Size([64, 384, 1, 1])\n",
      "- model.encoder.features.8.conv.3.weight: torch.Size([64])\n",
      "- model.encoder.features.8.conv.3.bias: torch.Size([64])\n",
      "- model.encoder.features.8.conv.3.running_mean: torch.Size([64])\n",
      "- model.encoder.features.8.conv.3.running_var: torch.Size([64])\n",
      "- model.encoder.features.8.conv.3.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.9.conv.0.0.weight: torch.Size([384, 64, 1, 1])\n",
      "- model.encoder.features.9.conv.0.1.weight: torch.Size([384])\n",
      "- model.encoder.features.9.conv.0.1.bias: torch.Size([384])\n",
      "- model.encoder.features.9.conv.0.1.running_mean: torch.Size([384])\n",
      "- model.encoder.features.9.conv.0.1.running_var: torch.Size([384])\n",
      "- model.encoder.features.9.conv.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.9.conv.1.0.weight: torch.Size([384, 1, 3, 3])\n",
      "- model.encoder.features.9.conv.1.1.weight: torch.Size([384])\n",
      "- model.encoder.features.9.conv.1.1.bias: torch.Size([384])\n",
      "- model.encoder.features.9.conv.1.1.running_mean: torch.Size([384])\n",
      "- model.encoder.features.9.conv.1.1.running_var: torch.Size([384])\n",
      "- model.encoder.features.9.conv.1.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.9.conv.2.weight: torch.Size([64, 384, 1, 1])\n",
      "- model.encoder.features.9.conv.3.weight: torch.Size([64])\n",
      "- model.encoder.features.9.conv.3.bias: torch.Size([64])\n",
      "- model.encoder.features.9.conv.3.running_mean: torch.Size([64])\n",
      "- model.encoder.features.9.conv.3.running_var: torch.Size([64])\n",
      "- model.encoder.features.9.conv.3.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.10.conv.0.0.weight: torch.Size([384, 64, 1, 1])\n",
      "- model.encoder.features.10.conv.0.1.weight: torch.Size([384])\n",
      "- model.encoder.features.10.conv.0.1.bias: torch.Size([384])\n",
      "- model.encoder.features.10.conv.0.1.running_mean: torch.Size([384])\n",
      "- model.encoder.features.10.conv.0.1.running_var: torch.Size([384])\n",
      "- model.encoder.features.10.conv.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.10.conv.1.0.weight: torch.Size([384, 1, 3, 3])\n",
      "- model.encoder.features.10.conv.1.1.weight: torch.Size([384])\n",
      "- model.encoder.features.10.conv.1.1.bias: torch.Size([384])\n",
      "- model.encoder.features.10.conv.1.1.running_mean: torch.Size([384])\n",
      "- model.encoder.features.10.conv.1.1.running_var: torch.Size([384])\n",
      "- model.encoder.features.10.conv.1.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.10.conv.2.weight: torch.Size([64, 384, 1, 1])\n",
      "- model.encoder.features.10.conv.3.weight: torch.Size([64])\n",
      "- model.encoder.features.10.conv.3.bias: torch.Size([64])\n",
      "- model.encoder.features.10.conv.3.running_mean: torch.Size([64])\n",
      "- model.encoder.features.10.conv.3.running_var: torch.Size([64])\n",
      "- model.encoder.features.10.conv.3.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.11.conv.0.0.weight: torch.Size([384, 64, 1, 1])\n",
      "- model.encoder.features.11.conv.0.1.weight: torch.Size([384])\n",
      "- model.encoder.features.11.conv.0.1.bias: torch.Size([384])\n",
      "- model.encoder.features.11.conv.0.1.running_mean: torch.Size([384])\n",
      "- model.encoder.features.11.conv.0.1.running_var: torch.Size([384])\n",
      "- model.encoder.features.11.conv.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.11.conv.1.0.weight: torch.Size([384, 1, 3, 3])\n",
      "- model.encoder.features.11.conv.1.1.weight: torch.Size([384])\n",
      "- model.encoder.features.11.conv.1.1.bias: torch.Size([384])\n",
      "- model.encoder.features.11.conv.1.1.running_mean: torch.Size([384])\n",
      "- model.encoder.features.11.conv.1.1.running_var: torch.Size([384])\n",
      "- model.encoder.features.11.conv.1.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.11.conv.2.weight: torch.Size([96, 384, 1, 1])\n",
      "- model.encoder.features.11.conv.3.weight: torch.Size([96])\n",
      "- model.encoder.features.11.conv.3.bias: torch.Size([96])\n",
      "- model.encoder.features.11.conv.3.running_mean: torch.Size([96])\n",
      "- model.encoder.features.11.conv.3.running_var: torch.Size([96])\n",
      "- model.encoder.features.11.conv.3.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.12.conv.0.0.weight: torch.Size([576, 96, 1, 1])\n",
      "- model.encoder.features.12.conv.0.1.weight: torch.Size([576])\n",
      "- model.encoder.features.12.conv.0.1.bias: torch.Size([576])\n",
      "- model.encoder.features.12.conv.0.1.running_mean: torch.Size([576])\n",
      "- model.encoder.features.12.conv.0.1.running_var: torch.Size([576])\n",
      "- model.encoder.features.12.conv.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.12.conv.1.0.weight: torch.Size([576, 1, 3, 3])\n",
      "- model.encoder.features.12.conv.1.1.weight: torch.Size([576])\n",
      "- model.encoder.features.12.conv.1.1.bias: torch.Size([576])\n",
      "- model.encoder.features.12.conv.1.1.running_mean: torch.Size([576])\n",
      "- model.encoder.features.12.conv.1.1.running_var: torch.Size([576])\n",
      "- model.encoder.features.12.conv.1.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.12.conv.2.weight: torch.Size([96, 576, 1, 1])\n",
      "- model.encoder.features.12.conv.3.weight: torch.Size([96])\n",
      "- model.encoder.features.12.conv.3.bias: torch.Size([96])\n",
      "- model.encoder.features.12.conv.3.running_mean: torch.Size([96])\n",
      "- model.encoder.features.12.conv.3.running_var: torch.Size([96])\n",
      "- model.encoder.features.12.conv.3.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.13.conv.0.0.weight: torch.Size([576, 96, 1, 1])\n",
      "- model.encoder.features.13.conv.0.1.weight: torch.Size([576])\n",
      "- model.encoder.features.13.conv.0.1.bias: torch.Size([576])\n",
      "- model.encoder.features.13.conv.0.1.running_mean: torch.Size([576])\n",
      "- model.encoder.features.13.conv.0.1.running_var: torch.Size([576])\n",
      "- model.encoder.features.13.conv.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.13.conv.1.0.weight: torch.Size([576, 1, 3, 3])\n",
      "- model.encoder.features.13.conv.1.1.weight: torch.Size([576])\n",
      "- model.encoder.features.13.conv.1.1.bias: torch.Size([576])\n",
      "- model.encoder.features.13.conv.1.1.running_mean: torch.Size([576])\n",
      "- model.encoder.features.13.conv.1.1.running_var: torch.Size([576])\n",
      "- model.encoder.features.13.conv.1.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.13.conv.2.weight: torch.Size([96, 576, 1, 1])\n",
      "- model.encoder.features.13.conv.3.weight: torch.Size([96])\n",
      "- model.encoder.features.13.conv.3.bias: torch.Size([96])\n",
      "- model.encoder.features.13.conv.3.running_mean: torch.Size([96])\n",
      "- model.encoder.features.13.conv.3.running_var: torch.Size([96])\n",
      "- model.encoder.features.13.conv.3.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.14.conv.0.0.weight: torch.Size([576, 96, 1, 1])\n",
      "- model.encoder.features.14.conv.0.1.weight: torch.Size([576])\n",
      "- model.encoder.features.14.conv.0.1.bias: torch.Size([576])\n",
      "- model.encoder.features.14.conv.0.1.running_mean: torch.Size([576])\n",
      "- model.encoder.features.14.conv.0.1.running_var: torch.Size([576])\n",
      "- model.encoder.features.14.conv.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.14.conv.1.0.weight: torch.Size([576, 1, 3, 3])\n",
      "- model.encoder.features.14.conv.1.1.weight: torch.Size([576])\n",
      "- model.encoder.features.14.conv.1.1.bias: torch.Size([576])\n",
      "- model.encoder.features.14.conv.1.1.running_mean: torch.Size([576])\n",
      "- model.encoder.features.14.conv.1.1.running_var: torch.Size([576])\n",
      "- model.encoder.features.14.conv.1.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.14.conv.2.weight: torch.Size([160, 576, 1, 1])\n",
      "- model.encoder.features.14.conv.3.weight: torch.Size([160])\n",
      "- model.encoder.features.14.conv.3.bias: torch.Size([160])\n",
      "- model.encoder.features.14.conv.3.running_mean: torch.Size([160])\n",
      "- model.encoder.features.14.conv.3.running_var: torch.Size([160])\n",
      "- model.encoder.features.14.conv.3.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.15.conv.0.0.weight: torch.Size([960, 160, 1, 1])\n",
      "- model.encoder.features.15.conv.0.1.weight: torch.Size([960])\n",
      "- model.encoder.features.15.conv.0.1.bias: torch.Size([960])\n",
      "- model.encoder.features.15.conv.0.1.running_mean: torch.Size([960])\n",
      "- model.encoder.features.15.conv.0.1.running_var: torch.Size([960])\n",
      "- model.encoder.features.15.conv.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.15.conv.1.0.weight: torch.Size([960, 1, 3, 3])\n",
      "- model.encoder.features.15.conv.1.1.weight: torch.Size([960])\n",
      "- model.encoder.features.15.conv.1.1.bias: torch.Size([960])\n",
      "- model.encoder.features.15.conv.1.1.running_mean: torch.Size([960])\n",
      "- model.encoder.features.15.conv.1.1.running_var: torch.Size([960])\n",
      "- model.encoder.features.15.conv.1.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.15.conv.2.weight: torch.Size([160, 960, 1, 1])\n",
      "- model.encoder.features.15.conv.3.weight: torch.Size([160])\n",
      "- model.encoder.features.15.conv.3.bias: torch.Size([160])\n",
      "- model.encoder.features.15.conv.3.running_mean: torch.Size([160])\n",
      "- model.encoder.features.15.conv.3.running_var: torch.Size([160])\n",
      "- model.encoder.features.15.conv.3.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.16.conv.0.0.weight: torch.Size([960, 160, 1, 1])\n",
      "- model.encoder.features.16.conv.0.1.weight: torch.Size([960])\n",
      "- model.encoder.features.16.conv.0.1.bias: torch.Size([960])\n",
      "- model.encoder.features.16.conv.0.1.running_mean: torch.Size([960])\n",
      "- model.encoder.features.16.conv.0.1.running_var: torch.Size([960])\n",
      "- model.encoder.features.16.conv.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.16.conv.1.0.weight: torch.Size([960, 1, 3, 3])\n",
      "- model.encoder.features.16.conv.1.1.weight: torch.Size([960])\n",
      "- model.encoder.features.16.conv.1.1.bias: torch.Size([960])\n",
      "- model.encoder.features.16.conv.1.1.running_mean: torch.Size([960])\n",
      "- model.encoder.features.16.conv.1.1.running_var: torch.Size([960])\n",
      "- model.encoder.features.16.conv.1.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.16.conv.2.weight: torch.Size([160, 960, 1, 1])\n",
      "- model.encoder.features.16.conv.3.weight: torch.Size([160])\n",
      "- model.encoder.features.16.conv.3.bias: torch.Size([160])\n",
      "- model.encoder.features.16.conv.3.running_mean: torch.Size([160])\n",
      "- model.encoder.features.16.conv.3.running_var: torch.Size([160])\n",
      "- model.encoder.features.16.conv.3.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.17.conv.0.0.weight: torch.Size([960, 160, 1, 1])\n",
      "- model.encoder.features.17.conv.0.1.weight: torch.Size([960])\n",
      "- model.encoder.features.17.conv.0.1.bias: torch.Size([960])\n",
      "- model.encoder.features.17.conv.0.1.running_mean: torch.Size([960])\n",
      "- model.encoder.features.17.conv.0.1.running_var: torch.Size([960])\n",
      "- model.encoder.features.17.conv.0.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.17.conv.1.0.weight: torch.Size([960, 1, 3, 3])\n",
      "- model.encoder.features.17.conv.1.1.weight: torch.Size([960])\n",
      "- model.encoder.features.17.conv.1.1.bias: torch.Size([960])\n",
      "- model.encoder.features.17.conv.1.1.running_mean: torch.Size([960])\n",
      "- model.encoder.features.17.conv.1.1.running_var: torch.Size([960])\n",
      "- model.encoder.features.17.conv.1.1.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.17.conv.2.weight: torch.Size([320, 960, 1, 1])\n",
      "- model.encoder.features.17.conv.3.weight: torch.Size([320])\n",
      "- model.encoder.features.17.conv.3.bias: torch.Size([320])\n",
      "- model.encoder.features.17.conv.3.running_mean: torch.Size([320])\n",
      "- model.encoder.features.17.conv.3.running_var: torch.Size([320])\n",
      "- model.encoder.features.17.conv.3.num_batches_tracked: torch.Size([])\n",
      "- model.encoder.features.18.0.weight: torch.Size([1280, 320, 1, 1])\n",
      "- model.encoder.features.18.1.weight: torch.Size([1280])\n",
      "- model.encoder.features.18.1.bias: torch.Size([1280])\n",
      "- model.encoder.features.18.1.running_mean: torch.Size([1280])\n",
      "- model.encoder.features.18.1.running_var: torch.Size([1280])\n",
      "- model.encoder.features.18.1.num_batches_tracked: torch.Size([])\n",
      "- model.decoder.blocks.0.conv1.0.weight: torch.Size([256, 1376, 3, 3])\n",
      "- model.decoder.blocks.0.conv1.1.weight: torch.Size([256])\n",
      "- model.decoder.blocks.0.conv1.1.bias: torch.Size([256])\n",
      "- model.decoder.blocks.0.conv1.1.running_mean: torch.Size([256])\n",
      "- model.decoder.blocks.0.conv1.1.running_var: torch.Size([256])\n",
      "- model.decoder.blocks.0.conv1.1.num_batches_tracked: torch.Size([])\n",
      "- model.decoder.blocks.0.conv2.0.weight: torch.Size([256, 256, 3, 3])\n",
      "- model.decoder.blocks.0.conv2.1.weight: torch.Size([256])\n",
      "- model.decoder.blocks.0.conv2.1.bias: torch.Size([256])\n",
      "- model.decoder.blocks.0.conv2.1.running_mean: torch.Size([256])\n",
      "- model.decoder.blocks.0.conv2.1.running_var: torch.Size([256])\n",
      "- model.decoder.blocks.0.conv2.1.num_batches_tracked: torch.Size([])\n",
      "- model.decoder.blocks.1.conv1.0.weight: torch.Size([128, 288, 3, 3])\n",
      "- model.decoder.blocks.1.conv1.1.weight: torch.Size([128])\n",
      "- model.decoder.blocks.1.conv1.1.bias: torch.Size([128])\n",
      "- model.decoder.blocks.1.conv1.1.running_mean: torch.Size([128])\n",
      "- model.decoder.blocks.1.conv1.1.running_var: torch.Size([128])\n",
      "- model.decoder.blocks.1.conv1.1.num_batches_tracked: torch.Size([])\n",
      "- model.decoder.blocks.1.conv2.0.weight: torch.Size([128, 128, 3, 3])\n",
      "- model.decoder.blocks.1.conv2.1.weight: torch.Size([128])\n",
      "- model.decoder.blocks.1.conv2.1.bias: torch.Size([128])\n",
      "- model.decoder.blocks.1.conv2.1.running_mean: torch.Size([128])\n",
      "- model.decoder.blocks.1.conv2.1.running_var: torch.Size([128])\n",
      "- model.decoder.blocks.1.conv2.1.num_batches_tracked: torch.Size([])\n",
      "- model.decoder.blocks.2.conv1.0.weight: torch.Size([64, 152, 3, 3])\n",
      "- model.decoder.blocks.2.conv1.1.weight: torch.Size([64])\n",
      "- model.decoder.blocks.2.conv1.1.bias: torch.Size([64])\n",
      "- model.decoder.blocks.2.conv1.1.running_mean: torch.Size([64])\n",
      "- model.decoder.blocks.2.conv1.1.running_var: torch.Size([64])\n",
      "- model.decoder.blocks.2.conv1.1.num_batches_tracked: torch.Size([])\n",
      "- model.decoder.blocks.2.conv2.0.weight: torch.Size([64, 64, 3, 3])\n",
      "- model.decoder.blocks.2.conv2.1.weight: torch.Size([64])\n",
      "- model.decoder.blocks.2.conv2.1.bias: torch.Size([64])\n",
      "- model.decoder.blocks.2.conv2.1.running_mean: torch.Size([64])\n",
      "- model.decoder.blocks.2.conv2.1.running_var: torch.Size([64])\n",
      "- model.decoder.blocks.2.conv2.1.num_batches_tracked: torch.Size([])\n",
      "- model.decoder.blocks.3.conv1.0.weight: torch.Size([32, 80, 3, 3])\n",
      "- model.decoder.blocks.3.conv1.1.weight: torch.Size([32])\n",
      "- model.decoder.blocks.3.conv1.1.bias: torch.Size([32])\n",
      "- model.decoder.blocks.3.conv1.1.running_mean: torch.Size([32])\n",
      "- model.decoder.blocks.3.conv1.1.running_var: torch.Size([32])\n",
      "- model.decoder.blocks.3.conv1.1.num_batches_tracked: torch.Size([])\n",
      "- model.decoder.blocks.3.conv2.0.weight: torch.Size([32, 32, 3, 3])\n",
      "- model.decoder.blocks.3.conv2.1.weight: torch.Size([32])\n",
      "- model.decoder.blocks.3.conv2.1.bias: torch.Size([32])\n",
      "- model.decoder.blocks.3.conv2.1.running_mean: torch.Size([32])\n",
      "- model.decoder.blocks.3.conv2.1.running_var: torch.Size([32])\n",
      "- model.decoder.blocks.3.conv2.1.num_batches_tracked: torch.Size([])\n",
      "- model.decoder.blocks.4.conv1.0.weight: torch.Size([16, 32, 3, 3])\n",
      "- model.decoder.blocks.4.conv1.1.weight: torch.Size([16])\n",
      "- model.decoder.blocks.4.conv1.1.bias: torch.Size([16])\n",
      "- model.decoder.blocks.4.conv1.1.running_mean: torch.Size([16])\n",
      "- model.decoder.blocks.4.conv1.1.running_var: torch.Size([16])\n",
      "- model.decoder.blocks.4.conv1.1.num_batches_tracked: torch.Size([])\n",
      "- model.decoder.blocks.4.conv2.0.weight: torch.Size([16, 16, 3, 3])\n",
      "- model.decoder.blocks.4.conv2.1.weight: torch.Size([16])\n",
      "- model.decoder.blocks.4.conv2.1.bias: torch.Size([16])\n",
      "- model.decoder.blocks.4.conv2.1.running_mean: torch.Size([16])\n",
      "- model.decoder.blocks.4.conv2.1.running_var: torch.Size([16])\n",
      "- model.decoder.blocks.4.conv2.1.num_batches_tracked: torch.Size([])\n",
      "- model.segmentation_head.0.weight: torch.Size([2, 16, 3, 3])\n",
      "- model.segmentation_head.0.bias: torch.Size([2])\n",
      "\n",
      "=== Hyperparameters ===\n",
      "- num_classes: 2\n",
      "- learning_rate: 0.01\n",
      "- weight_decay: 0.0001\n",
      "- pretrained_path: None\n",
      "- output_stride: 16\n",
      "- arch: Unet\n",
      "- encoder_name: mobilenet_v2\n",
      "- classes: 2\n",
      "\n",
      "=== Other Checkpoint Info ===\n",
      "- epoch: 44\n",
      "- global_step: 9405\n",
      "- pytorch-lightning_version: 2.5.2\n",
      "- loops: <class 'dict'> with 4 items\n",
      "- callbacks: <class 'dict'> with 1 items\n",
      "- optimizer_states: <class 'list'>\n",
      "- lr_schedulers: <class 'list'>\n",
      "- MixedPrecision: <class 'dict'> with 5 items\n",
      "- hparams_name: kwargs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path = \"/home/treerspeaking/src/python/hand_seg/logs/hand_segmentation/version_57/checkpoints/best_hand_segmentation_epoch=44_val/epoch_iou=0.0000.ckpt\"\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "print(\"=== Checkpoint Keys ===\")\n",
    "for key in checkpoint.keys():\n",
    "    print(f\"- {key}\")\n",
    "\n",
    "print(\"\\n=== State Dict Keys (Model Weights) ===\")\n",
    "if 'state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['state_dict']\n",
    "    for key in state_dict.keys():\n",
    "        print(f\"- {key}: {state_dict[key].shape}\")\n",
    "\n",
    "print(\"\\n=== Hyperparameters ===\")\n",
    "if 'hyper_parameters' in checkpoint:\n",
    "    hyper_params = checkpoint['hyper_parameters']\n",
    "    for key, value in hyper_params.items():\n",
    "        print(f\"- {key}: {value}\")\n",
    "\n",
    "print(\"\\n=== Other Checkpoint Info ===\")\n",
    "for key in checkpoint.keys():\n",
    "    if key not in ['state_dict', 'hyper_parameters']:\n",
    "        value = checkpoint[key]\n",
    "        if isinstance(value, (int, float, str, bool)):\n",
    "            print(f\"- {key}: {value}\")\n",
    "        elif isinstance(value, dict):\n",
    "            print(f\"- {key}: {type(value)} with {len(value)} items\")\n",
    "        else:\n",
    "            print(f\"- {key}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe498acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import DeepLabLightningModule, HandSegDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3728defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = HandSegDataModule(\"./dataset/new_dataset\", 24, 24, None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76fb9a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['masks_hand_signature', 'masks_seal']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.train.mask_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4397468a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hand_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
